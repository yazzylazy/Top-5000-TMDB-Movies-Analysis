{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI 4142 Deliverable 2 - Phase 2\n",
    "### Group 35\n",
    "- Yasin Elmi, 300163765\n",
    "- Oluwatobiloba Ogunbi, 300202843\n",
    "- Michael Thompson, 300175414"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intial reading of files into dataframes\n",
    "\n",
    "Note: may need to adjust the file name and/or location if this notebook is not within the same directory as the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df_movies = pd.read_csv('datasets/tmdb_5000_movies.csv', index_col=False)\n",
    "df_credits = pd.read_csv('datasets/tmdb_5000_movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming of data\n",
    "The following section is the process of our data being cleaned and transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping columns\n",
    "Initial dropping of unnecessary columns, according to our first deliverable plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unneccessary columns\n",
    "df_movies = df_movies.drop('homepage', axis=1)\n",
    "df_movies = df_movies.drop('id', axis=1)\n",
    "df_movies = df_movies.drop('keywords', axis=1)\n",
    "df_movies = df_movies.drop('overview', axis=1)\n",
    "df_movies = df_movies.drop('status', axis=1)\n",
    "df_movies = df_movies.drop('tagline', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning attributes containing JSON objects as String\n",
    "The 'genre', 'production_companies' and 'production_countries' attributes contain a JSON as a string, which contains a desired key-value pair. A function was created to extract the desired value within the string, and return it as a list.\n",
    "\n",
    "### TODO\n",
    "Some of the rows contain an empty value, denoted by the empty square brackets []. Must decide on how to handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the genres, production_companies and production_countries columns from json string\n",
    "\n",
    "def extract_json_key(string, key):\n",
    "    '''\n",
    "    Helper function used to translate string to list of dictionaries, and extract the 'name' key-value pair\n",
    "    (Str, Str) -> Str\n",
    "    Preconditions: None\n",
    "    '''\n",
    "    clean_list = json.loads(string)\n",
    "    name = [x[key] for x in clean_list]\n",
    "    return ', '.join(name)\n",
    "\n",
    "# Calling the above function to extract the respective 'name' key-value \n",
    "df_movies['genres'] = df_movies['genres'].map(lambda x: extract_json_key(x, 'name'))\n",
    "df_movies['production_companies'] = df_movies['production_companies'].map(lambda x: extract_json_key(x, 'name'))\n",
    "df_movies['production_countries'] = df_movies['production_countries'].map(lambda x: extract_json_key(x, 'name'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the language attributes\n",
    "This section is focused on cleaning the two langauge attribtues, 'original_language' and 'spoken_language'. \n",
    "\n",
    "'original_language' is originally written in ISO 639-1 code, which is a 2 letter abbreviation of a word. Using a dictionary found below, the word is swapped for it's verbose display.\n",
    "\n",
    "'spoken_language' is a little more difficult, where a film can have multiple spoken languages. This is stored in a similar JSON string as above which stores both the ISO 639-1 code and the full written word. Unfortunately, the full written word may utilize a unicode character (i.e. 'French' written in French has an accent, which stores a unicode character). The solution to this is utililizing our previously made dictionary and search it using the provided ISO 639-1 code. A function takes the list of JSON objects, extracts the ISO 639-1 code and translates it, then returns.\n",
    "\n",
    "### TODO:\n",
    "- Some of the columns are empty (denoted by an empty set of square brackets []), an example of this is column k row 2650. Need to make decision on how to handle this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary mapping 2 letter language abbrv. to full word\n",
    "language_two_code_dict = {\n",
    "    'en': 'English',\n",
    "    'fr': 'French',\n",
    "    'ja': 'Japanese',\n",
    "    'zh': 'Chinese',\n",
    "    'es': 'Spanish',\n",
    "    'de': 'German',\n",
    "    'hi': 'Hindi',\n",
    "    'ru': 'Russian',\n",
    "    'ko': 'Korean',\n",
    "    'te': 'Telugu',\n",
    "    'cn': 'Chinese',\n",
    "    'it': 'Italian',\n",
    "    'nl': 'Dutch',\n",
    "    'ta': 'Tamil',\n",
    "    'sv': 'Swedish',\n",
    "    'th': 'Thai',\n",
    "    'da': 'Danish',\n",
    "    'xx': 'No Language',\n",
    "    'hu': 'Hungarian',\n",
    "    'cs': 'Czech',\n",
    "    'pt': 'Portuguese',\n",
    "    'is': 'Icelandic',\n",
    "    'tr': 'Turkish',\n",
    "    'nb': 'Norwegian Bokmal',\n",
    "    'af': 'Afrikaans',\n",
    "    'pl': 'Polish',\n",
    "    'he': 'Hebrew',\n",
    "    'ar': 'Arabic',\n",
    "    'vi': 'Vietnamese',\n",
    "    'ky': 'Kyrgyz',\n",
    "    'id': 'Indonesian',\n",
    "    'ro': 'Romanian',\n",
    "    'fa': 'Persian',\n",
    "    'no': 'Norwegian',\n",
    "    'sl': 'Slovenian',\n",
    "    'ps': 'Pashto',\n",
    "    'el': 'Greek'\n",
    "}\n",
    "\n",
    "def language_lookup(word):\n",
    "    '''\n",
    "    Helper function that searches a ISO 639-1 code in the dictionary and returns the verbose word\n",
    "    String -> String\n",
    "    '''\n",
    "    if(word in language_two_code_dict):\n",
    "        return language_two_code_dict.get(word)\n",
    "    else:\n",
    "        return 'NEED TO DOUBLE CHECK MISSING'\n",
    "\n",
    "df_movies['original_language'] = df_movies['original_language'].map(lambda x: language_lookup(x))\n",
    "\n",
    "\n",
    "def translate_and_clean(string):\n",
    "    '''\n",
    "    Helper function that takes string of 2 code abbreviations, translates and returns in a String\n",
    "    (Str) -> Str\n",
    "    '''\n",
    "    extract = extract_json_key(string, 'iso_639_1').split(',')\n",
    "    for i in range(len(extract)):\n",
    "        extract[i] = extract[i].strip()\n",
    "        extract[i] = language_lookup(extract[i])\n",
    "    \n",
    "    return ', '.join(extract)\n",
    "\n",
    "# Apply function on each row within 'spoken_languages' column\n",
    "df_movies['spoken_languages'] = df_movies['spoken_languages'].map(lambda x: translate_and_clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Staging the data\n",
    "Below is the staging part of our script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
